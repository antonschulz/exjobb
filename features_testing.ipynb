{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def compute_velocity(df):\n",
    "    \"\"\"\n",
    "    Compute instantaneous speed (velocity magnitude) assuming unit time intervals.\n",
    "    Velocity at step i = distance between point i and i+1.\n",
    "    \"\"\"\n",
    "    dx = np.diff(df['x'].values)\n",
    "    dy = np.diff(df['y'].values)\n",
    "    speed = np.sqrt(dx**2 + dy**2)\n",
    "    return speed\n",
    "\n",
    "def compute_acceleration(velocity):\n",
    "    \"\"\"\n",
    "    Compute acceleration as discrete time derivative of speed.\n",
    "    Acceleration at step i = velocity[i+1] - velocity[i].\n",
    "    \"\"\"\n",
    "    return np.diff(velocity)\n",
    "\n",
    "def compute_jerk(acceleration):\n",
    "    \"\"\"\n",
    "    Compute jerk as discrete time derivative of acceleration.\n",
    "    Jerk at step i = acceleration[i+1] - acceleration[i].\n",
    "    \"\"\"\n",
    "    return np.diff(acceleration)\n",
    "\n",
    "def compute_angle(df):\n",
    "    \"\"\"\n",
    "    Compute heading angle of movement vectors.\n",
    "    Angle at step i = arctan2(dy, dx) between consecutive points.\n",
    "    Unwrap to avoid discontinuities at ±π.\n",
    "    \"\"\"\n",
    "    dx = np.diff(df['x'].values)\n",
    "    dy = np.diff(df['y'].values)\n",
    "    angle = np.arctan2(dy, dx)\n",
    "    return np.unwrap(angle)\n",
    "\n",
    "def compute_angular_velocity(angle):\n",
    "    \"\"\"\n",
    "    Compute angular velocity as discrete time derivative of heading angle.\n",
    "    \"\"\"\n",
    "    return np.diff(angle)\n",
    "\n",
    "def compute_angular_acceleration(angular_velocity):\n",
    "    \"\"\"\n",
    "    Compute angular acceleration as discrete time derivative of angular velocity.\n",
    "    \"\"\"\n",
    "    return np.diff(angular_velocity)\n",
    "\n",
    "def compute_straightness(df):\n",
    "    \"\"\"\n",
    "    Compute local straightness over sliding windows of 3 points.\n",
    "    \"\"\"\n",
    "    x, y = df['x'].values, df['y'].values\n",
    "    n = len(x)\n",
    "    s = []\n",
    "    for i in range(n - 2):\n",
    "        p0, p1, p2 = np.array([x[i], y[i]]), np.array([x[i+1], y[i+1]]), np.array([x[i+2], y[i+2]])\n",
    "        disp = np.linalg.norm(p2 - p0)\n",
    "        path = np.linalg.norm(p1 - p0) + np.linalg.norm(p2 - p1)\n",
    "        s.append(disp / path if path != 0 else np.nan)\n",
    "    return np.array(s)\n",
    "\n",
    "\n",
    "def compute_curvature(df, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Estimate curvature at each point using discrete derivatives:\n",
    "      k = |x' y'' - y' x''| / (x'^2 + y'^2)^(3/2)\n",
    "    \n",
    "    To avoid division by zero, we:\n",
    "      1) compute denom = (dx^2 + dy^2)**1.5\n",
    "      2) replace any denom < eps with eps\n",
    "      3) compute k safely\n",
    "      4) set curvature to 0 where the movement is truly stationary\n",
    "    \"\"\"\n",
    "    x = df['x'].values\n",
    "    y = df['y'].values\n",
    "\n",
    "    # first derivatives\n",
    "    dx = np.gradient(x)\n",
    "    dy = np.gradient(y)\n",
    "    # second derivatives\n",
    "    ddx = np.gradient(dx)\n",
    "    ddy = np.gradient(dy)\n",
    "\n",
    "    # numerator of curvature formula\n",
    "    num = np.abs(dx * ddy - dy * ddx)\n",
    "    # denominator, with small epsilon floor\n",
    "    denom = (dx**2 + dy**2)**1.5\n",
    "    denom = np.where(denom < eps, eps, denom)\n",
    "\n",
    "    k = num / denom\n",
    "\n",
    "    return k\n",
    "\n",
    "def compute_stillness(df):\n",
    "    \"\"\"\n",
    "    Count timesteps where the insect is still:\n",
    "    Positions where both x and y do not change from previous frame.\n",
    "    \"\"\"\n",
    "    dx = np.diff(df['x'].values)\n",
    "    dy = np.diff(df['y'].values)\n",
    "    # Still when no movement in both axes\n",
    "    return int(np.sum((dx == 0) & (dy == 0)))\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "def compute_step_length_stats(df):\n",
    "    \"\"\"\n",
    "    Compute statistics of the step-length distribution (distances between successive points).\n",
    "    Returns a dict with:\n",
    "      - mean: average step length\n",
    "      - variance: variance of step lengths\n",
    "      - skewness: measure of asymmetry of the distribution\n",
    "      - kurtosis: excess kurtosis (peakedness relative to normal)\n",
    "    \"\"\"\n",
    "    dx = np.diff(df['x'].values)\n",
    "    dy = np.diff(df['y'].values)\n",
    "    steps = np.sqrt(dx**2 + dy**2)\n",
    "    μ = np.mean(steps)\n",
    "    σ = np.std(steps)\n",
    "    skew = np.mean((steps - μ)**3) / σ**3 if σ > 0 else np.nan\n",
    "    kurt = np.mean((steps - μ)**4) / σ**4 - 3 if σ > 0 else np.nan\n",
    "    return {\n",
    "        'step_mean': μ,\n",
    "        'step_variance': σ**2,\n",
    "        'step_skewness': skew,\n",
    "        'step_kurtosis': kurt\n",
    "    }\n",
    "\n",
    "def compute_turning_angle_stats(df):\n",
    "    \"\"\"\n",
    "    Compute turning-angle features:\n",
    "      - mean_resultant_length: directional persistence\n",
    "      - var, skewness, kurtosis of the turn-angle distribution\n",
    "    Angles are wrapped to [-π, π].\n",
    "    \"\"\"\n",
    "    dx = np.diff(df['x'].values)\n",
    "    dy = np.diff(df['y'].values)\n",
    "    angles = np.arctan2(dy, dx)\n",
    "    turns = np.diff(angles)\n",
    "    # wrap turns into [-π,π]\n",
    "    turns = (turns + np.pi) % (2 * np.pi) - np.pi\n",
    "    \n",
    "    n = len(turns)\n",
    "    cr = np.sum(np.cos(turns)) / n\n",
    "    sr = np.sum(np.sin(turns)) / n\n",
    "    mean_res_len = np.sqrt(cr**2 + sr**2)\n",
    "    \n",
    "    μ = np.mean(turns)\n",
    "    σ = np.std(turns)\n",
    "    skew = np.mean((turns - μ)**3) / σ**3 if σ > 0 else np.nan\n",
    "    kurt = np.mean((turns - μ)**4) / σ**4 - 3 if σ > 0 else np.nan\n",
    "    \n",
    "    return {\n",
    "        'turn_mrl': mean_res_len,\n",
    "        'turn_variance': σ**2,\n",
    "        'turn_skewness': skew,\n",
    "        'turn_kurtosis': kurt\n",
    "    }\n",
    "\n",
    "def compute_pause_bout_stats(df, speed_thresh=1e-2):\n",
    "    \"\"\"\n",
    "    Detect pause bouts where instantaneous speed < speed_thresh.\n",
    "    Returns:\n",
    "      - n_pauses: number of pause bouts\n",
    "      - mean_pause_duration: average frames per pause\n",
    "      - median_pause_duration\n",
    "      - fraction_paused: total paused frames / total steps\n",
    "      - pause_to_move_rate: transitions from pause to move per frame\n",
    "      - move_to_pause_rate: transitions from move to pause per frame\n",
    "    \"\"\"\n",
    "    dx = np.diff(df['x'].values)\n",
    "    dy = np.diff(df['y'].values)\n",
    "    speed = np.sqrt(dx**2 + dy**2)\n",
    "    paused = speed < speed_thresh\n",
    "    \n",
    "    # find transitions\n",
    "    trans = np.diff(paused.astype(int))\n",
    "    starts = np.where(trans == 1)[0] + 1  # move→pause\n",
    "    ends   = np.where(trans == -1)[0] + 1 # pause→move\n",
    "    \n",
    "    # handle edge cases if track starts or ends paused\n",
    "    if paused[0]:\n",
    "        starts = np.insert(starts, 0, 0)\n",
    "    if paused[-1]:\n",
    "        ends = np.append(ends, len(paused))\n",
    "    \n",
    "    durations = ends - starts\n",
    "    n_pauses = len(durations)\n",
    "    \n",
    "    return {\n",
    "        'n_pauses': n_pauses,\n",
    "        'mean_pause_dur': np.mean(durations) if n_pauses > 0 else 0,\n",
    "        'median_pause_dur': np.median(durations) if n_pauses > 0 else 0,\n",
    "        'fraction_paused': np.sum(paused) / len(paused),\n",
    "        'pause_to_move_rate': len(ends) / len(paused),\n",
    "        'move_to_pause_rate': len(starts) / len(paused)\n",
    "    }\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial import ConvexHull, QhullError\n",
    "\n",
    "def compute_convex_hull_area(df):\n",
    "    \"\"\"\n",
    "    Compute the area of the convex hull enclosing the track points.\n",
    "    Returns 0.0 if fewer than 3 unique points.\n",
    "    \"\"\"\n",
    "    pts = np.vstack([df['x'].values, df['y'].values]).T\n",
    "\n",
    "    # need at least 3 total points *and* 3 unique points\n",
    "    if len(pts) < 3:\n",
    "        return 0.0\n",
    "\n",
    "    # filter out duplicates\n",
    "    unique_pts = np.unique(pts, axis=0)\n",
    "    if unique_pts.shape[0] < 3:\n",
    "        return 0.0\n",
    "\n",
    "    # now it’s safe to compute\n",
    "    try:\n",
    "        hull = ConvexHull(unique_pts)\n",
    "        return hull.volume\n",
    "    except QhullError:\n",
    "        # just in case some other degeneracy slips through\n",
    "        return 0.0\n",
    "\n",
    "def compute_radius_of_gyration(df):\n",
    "    \"\"\"\n",
    "    Compute the radius of gyration:\n",
    "    sqrt(mean((x - x_mean)^2 + (y - y_mean)^2))\n",
    "    Measures how tightly the path clusters around its centroid.\n",
    "    \"\"\"\n",
    "    x = df['x'].values\n",
    "    y = df['y'].values\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    return np.sqrt(np.mean((x - x_mean)**2 + (y - y_mean)**2))\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "def summarize(arr, name):\n",
    "    \"\"\"\n",
    "    Compute summary statistics for an array:\n",
    "    - mean\n",
    "    - median\n",
    "    - minimum\n",
    "    - maximum\n",
    "    - variance\n",
    "\n",
    "    If arr is empty or all NaNs, every statistic will be np.nan.\n",
    "    \"\"\"\n",
    "    # Drop NaNs\n",
    "    arr_clean = arr[~np.isnan(arr)]\n",
    "    \n",
    "    # If no valid data, return all NaNs\n",
    "    if arr_clean.size == 0:\n",
    "        return {\n",
    "            f\"{name}_mean\":     np.nan,\n",
    "            f\"{name}_median\":   np.nan,\n",
    "            f\"{name}_min\":      np.nan,\n",
    "            f\"{name}_max\":      np.nan,\n",
    "            f\"{name}_variance\": np.nan,\n",
    "        }\n",
    "    \n",
    "    # Otherwise compute stats safely\n",
    "    μ        = np.nanmean(arr_clean)\n",
    "    median   = np.nanmedian(arr_clean)\n",
    "    _min     = np.nanmin(arr_clean)\n",
    "    _max     = np.nanmax(arr_clean)\n",
    "    variance = np.nanvar(arr_clean)\n",
    "    \n",
    "    return {\n",
    "        f\"{name}_mean\":     μ,\n",
    "        f\"{name}_median\":   median,\n",
    "        f\"{name}_min\":      _min,\n",
    "        f\"{name}_max\":      _max,\n",
    "        f\"{name}_variance\": variance,\n",
    "    }\n",
    "\n",
    "def extract_features(df):\n",
    "    \"\"\"\n",
    "    Compute summary statistics of kinematic and geometric features:\n",
    "    - velocity, acceleration, jerk\n",
    "    - angle, angular velocity, angular acceleration\n",
    "    - straightness, curvature\n",
    "    - stillness count\n",
    "    - step-length distribution stats\n",
    "    - turning-angle distribution stats\n",
    "    - pause-bout metrics\n",
    "    - convex-hull area\n",
    "    - radius of gyration\n",
    "\n",
    "    Returns a pandas Series with all features.\n",
    "    \"\"\"\n",
    "    # 1) Raw kinematic/geometric arrays\n",
    "    vel       = compute_velocity(df)\n",
    "    acc       = compute_acceleration(vel)\n",
    "    jerk      = compute_jerk(acc)\n",
    "    angle     = compute_angle(df)\n",
    "    ang_vel   = compute_angular_velocity(angle)\n",
    "    ang_acc   = compute_angular_acceleration(ang_vel)\n",
    "    straight  = compute_straightness(df)\n",
    "    curvature = compute_curvature(df)\n",
    "    still_cnt = compute_stillness(df)\n",
    "\n",
    "    # 2) Summarize arrays (mean, median, min, max, variance, skewness, kurtosis)\n",
    "    features = {}\n",
    "    for arr, name in [\n",
    "        (vel, \"velocity\"), (acc, \"acceleration\"), (jerk, \"jerk\"),\n",
    "        (angle, \"angle\"), (ang_vel, \"angular_velocity\"),\n",
    "        (ang_acc, \"angular_acceleration\"),\n",
    "        (straight, \"straightness\"), (curvature, \"curvature\")\n",
    "    ]:\n",
    "        features.update(summarize(arr, name))\n",
    "\n",
    "    # 3) Add scalar features\n",
    "    features[\"stillness_count\"] = still_cnt\n",
    "\n",
    "    # 4) Add distributional stats\n",
    "    #features.update(compute_step_length_stats(df))\n",
    "    features.update(compute_turning_angle_stats(df))\n",
    "    #features.update(compute_pause_bout_stats(df))\n",
    "\n",
    "    # 5) Add spatial metrics\n",
    "    features[\"convex_hull_area\"]   = compute_convex_hull_area(df)\n",
    "    features[\"radius_of_gyration\"] = compute_radius_of_gyration(df)\n",
    "\n",
    "    return pd.Series(features)\n",
    "\n",
    "# Ensure compute_* helper functions remain as defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not find required columns in 0_81514540600.csv. Skipping file.\n",
      "\n",
      "=== RandomForest ===\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anton/miniconda3/envs/ann/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/anton/miniconda3/envs/ann/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/anton/miniconda3/envs/ann/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/anton/miniconda3/envs/ann/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/anton/miniconda3/envs/ann/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/anton/miniconda3/envs/ann/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/anton/miniconda3/envs/ann/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/anton/miniconda3/envs/ann/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/anton/miniconda3/envs/ann/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/anton/miniconda3/envs/ann/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/anton/miniconda3/envs/ann/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/anton/miniconda3/envs/ann/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/anton/miniconda3/envs/ann/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'clf__max_depth': 10, 'clf__min_samples_leaf': 4, 'clf__n_estimators': 100}\n",
      "Best CV f1_macro: 0.478\n",
      "\n",
      "Test Performance over 10 runs:\n",
      "Accuracy            : 0.768 ± 0.008\n",
      "Balanced Accuracy   : 0.523 ± 0.025\n",
      "Precision macro     : 0.544 ± 0.033\n",
      "Recall macro        : 0.523 ± 0.025\n",
      "F1 macro            : 0.512 ± 0.028\n",
      "\n",
      "Summed confusion matrix (counts summed over runs):\n",
      "[[2882   19   43  536]\n",
      " [ 109   24   15   12]\n",
      " [  77    4   81   18]\n",
      " [ 144   28    0  338]]\n",
      "\n",
      "Average confusion matrix (averaged over runs):\n",
      "[[288.2   1.9   4.3  53.6]\n",
      " [ 10.9   2.4   1.5   1.2]\n",
      " [  7.7   0.4   8.1   1.8]\n",
      " [ 14.4   2.8   0.   33.8]]\n",
      "\n",
      "=== XGBoost ===\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anton/miniconda3/envs/ann/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/anton/miniconda3/envs/ann/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/anton/miniconda3/envs/ann/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/anton/miniconda3/envs/ann/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/anton/miniconda3/envs/ann/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/anton/miniconda3/envs/ann/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'clf__learning_rate': 0.1, 'clf__max_depth': 6, 'clf__n_estimators': 300}\n",
      "Best CV f1_macro: 0.401\n",
      "\n",
      "Test Performance over 10 runs:\n",
      "Accuracy            : 0.827 ± 0.000\n",
      "Balanced Accuracy   : 0.409 ± 0.000\n",
      "Precision macro     : 0.745 ± 0.000\n",
      "Recall macro        : 0.409 ± 0.000\n",
      "F1 macro            : 0.476 ± 0.000\n",
      "\n",
      "Summed confusion matrix (counts summed over runs):\n",
      "[[3380   10    0   90]\n",
      " [ 130   30    0    0]\n",
      " [ 140    0   40    0]\n",
      " [ 370    0   10  130]]\n",
      "\n",
      "Average confusion matrix (averaged over runs):\n",
      "[[338.   1.   0.   9.]\n",
      " [ 13.   3.   0.   0.]\n",
      " [ 14.   0.   4.   0.]\n",
      " [ 37.   0.   1.  13.]]\n",
      "\n",
      "=== SVM ===\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best parameters: {'clf__C': 10, 'clf__gamma': 'scale'}\n",
      "Best CV f1_macro: 0.462\n",
      "\n",
      "Test Performance over 10 runs:\n",
      "Accuracy            : 0.767 ± 0.000\n",
      "Balanced Accuracy   : 0.592 ± 0.000\n",
      "Precision macro     : 0.502 ± 0.000\n",
      "Recall macro        : 0.592 ± 0.000\n",
      "F1 macro            : 0.536 ± 0.000\n",
      "\n",
      "Summed confusion matrix (counts summed over runs):\n",
      "[[2870  100  120  390]\n",
      " [  70   70   10   10]\n",
      " [  40   10  100   30]\n",
      " [ 150   70   10  280]]\n",
      "\n",
      "Average confusion matrix (averaged over runs):\n",
      "[[287.  10.  12.  39.]\n",
      " [  7.   7.   1.   1.]\n",
      " [  4.   1.  10.   3.]\n",
      " [ 15.   7.   1.  28.]]\n",
      "\n",
      "=== DecisionTree ===\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Best parameters: {'clf__max_depth': None, 'clf__min_samples_leaf': 2, 'clf__min_samples_split': 10}\n",
      "Best CV f1_macro: 0.404\n",
      "\n",
      "Test Performance over 10 runs:\n",
      "Accuracy            : 0.703 ± 0.006\n",
      "Balanced Accuracy   : 0.357 ± 0.007\n",
      "Precision macro     : 0.340 ± 0.007\n",
      "Recall macro        : 0.357 ± 0.007\n",
      "F1 macro            : 0.346 ± 0.007\n",
      "\n",
      "Summed confusion matrix (counts summed over runs):\n",
      "[[2809  145  167  359]\n",
      " [  95   12   20   33]\n",
      " [  89   48   30   13]\n",
      " [ 242   43   32  193]]\n",
      "\n",
      "Average confusion matrix (averaged over runs):\n",
      "[[280.9  14.5  16.7  35.9]\n",
      " [  9.5   1.2   2.    3.3]\n",
      " [  8.9   4.8   3.    1.3]\n",
      " [ 24.2   4.3   3.2  19.3]]\n",
      "\n",
      "=== KNN ===\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anton/miniconda3/envs/ann/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/anton/miniconda3/envs/ann/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'clf__n_neighbors': 3, 'clf__weights': 'distance'}\n",
      "Best CV f1_macro: 0.350\n",
      "\n",
      "Test Performance over 10 runs:\n",
      "Accuracy            : 0.788 ± 0.000\n",
      "Balanced Accuracy   : 0.369 ± 0.000\n",
      "Precision macro     : 0.491 ± 0.000\n",
      "Recall macro        : 0.369 ± 0.000\n",
      "F1 macro            : 0.400 ± 0.000\n",
      "\n",
      "Summed confusion matrix (counts summed over runs):\n",
      "[[3230    0   20  230]\n",
      " [ 120   20   10   10]\n",
      " [ 140    0   30   10]\n",
      " [ 330   30   20  130]]\n",
      "\n",
      "Average confusion matrix (averaged over runs):\n",
      "[[323.   0.   2.  23.]\n",
      " [ 12.   2.   1.   1.]\n",
      " [ 14.   0.   3.   1.]\n",
      " [ 33.   3.   2.  13.]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import clone\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "\n",
    "# 1) load your data\n",
    "from utils.data_loading import load_dataset\n",
    "train_data, val_data, test_data, full_training_data = load_dataset(\n",
    "    \"datasets/it-trajs_complete/\", diff=False, scaler=None\n",
    ")\n",
    "\n",
    "def build_X_y(dataset):\n",
    "    feats, labels = [], []\n",
    "    for fname, track in dataset.tracks_dict.items():\n",
    "        try:\n",
    "            feats.append(extract_features(track['dataframe']))\n",
    "        except Exception as E:\n",
    "            print(E)\n",
    "            print(f\"filename: {fname}\")\n",
    "            print(track)\n",
    "            exit()\n",
    "       \n",
    "        labels.append(int(track['label']))\n",
    "    X = pd.DataFrame(feats)\n",
    "    y = pd.Series(labels, name='label', dtype=int)\n",
    "    return X, y\n",
    "\n",
    "# 2) build feature matrices\n",
    "X_train, y_train = build_X_y(full_training_data)\n",
    "X_test,  y_test  = build_X_y(test_data)\n",
    "\n",
    "# 3) candidate models\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(random_state=42, n_jobs=-1, class_weight='balanced'),\n",
    "    'XGBoost':       XGBClassifier(eval_metric='logloss', random_state=42, n_jobs=-1),\n",
    "    'SVM':           SVC(kernel='rbf', probability=True, random_state=42, class_weight='balanced'),\n",
    "    'DecisionTree':  DecisionTreeClassifier(random_state=42, class_weight='balanced'),\n",
    "    'KNN':           KNeighborsClassifier(n_jobs=-1)\n",
    "}\n",
    "\n",
    "# 4) hyperparameter grids\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'clf__n_estimators':     [100, 300, 500],\n",
    "        'clf__max_depth':        [None, 10, 20],\n",
    "        'clf__min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'clf__n_estimators': [100, 300],\n",
    "        'clf__learning_rate':[0.01, 0.1],\n",
    "        'clf__max_depth':    [3, 6, 10]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'clf__C':     [0.1, 1, 10],\n",
    "        'clf__gamma': ['scale', 'auto']\n",
    "    },\n",
    "    'DecisionTree': {\n",
    "        'clf__max_depth':        [None, 10, 20],\n",
    "        'clf__min_samples_split':[2, 5, 10],\n",
    "        'clf__min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'KNN': {\n",
    "        'clf__n_neighbors': [3, 5, 7],\n",
    "        'clf__weights':     ['uniform', 'distance']\n",
    "    }\n",
    "}\n",
    "\n",
    "# 5) cross-validation setup\n",
    "cv      = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "\n",
    "# 6) tuning and repeated test evaluation\n",
    "n_repeats = 10\n",
    "seeds = list(range(n_repeats))\n",
    "\n",
    "for name, clf in models.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    # build pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler',  StandardScaler()),\n",
    "        ('clf',     clf)\n",
    "    ])\n",
    "    from sklearn.exceptions import UndefinedMetricWarning\n",
    "    import warnings\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "    # grid search\n",
    "    grid = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grids[name],\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        refit='f1_macro',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Best parameters: {grid.best_params_}\")\n",
    "    print(f\"Best CV f1_macro: {grid.best_score_:.3f}\")\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    # repeated test performance\n",
    "    accs, ba_accs, precs, recs, f1s = [], [], [], [], []\n",
    "    cms = []  # <-- collect confusion matrices\n",
    "\n",
    "    for seed in seeds:\n",
    "        model = clone(grid.best_estimator_)\n",
    "        try:\n",
    "            model.set_params(clf__random_state=seed)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # compute metrics\n",
    "        accs.append(accuracy_score(y_test, y_pred))\n",
    "        ba_accs.append(balanced_accuracy_score(y_test, y_pred))\n",
    "        precs.append(precision_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "        recs.append(recall_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "        f1s.append(f1_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "\n",
    "        # confusion matrix for this run\n",
    "        cms.append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # summarize repeated performance\n",
    "    print(f\"\\nTest Performance over {n_repeats} runs:\")\n",
    "    print(f\"Accuracy            : {np.mean(accs):.3f} ± {np.std(accs):.3f}\")\n",
    "    print(f\"Balanced Accuracy   : {np.mean(ba_accs):.3f} ± {np.std(ba_accs):.3f}\")\n",
    "    print(f\"Precision macro     : {np.mean(precs):.3f} ± {np.std(precs):.3f}\")\n",
    "    print(f\"Recall macro        : {np.mean(recs):.3f} ± {np.std(recs):.3f}\")\n",
    "    print(f\"F1 macro            : {np.mean(f1s):.3f} ± {np.std(f1s):.3f}\")\n",
    "\n",
    "    # now aggregate confusion matrices\n",
    "    sum_cm = sum(cms)           # total counts across all runs\n",
    "    avg_cm = sum_cm / len(cms)  # average per-run\n",
    "\n",
    "    print(\"\\nSummed confusion matrix (counts summed over runs):\")\n",
    "    print(sum_cm)\n",
    "\n",
    "    print(\"\\nAverage confusion matrix (averaged over runs):\")\n",
    "    print(np.round(avg_cm, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# 1) Fit your RandomForest pipeline on the full training data\n",
    "rf_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler',  StandardScaler()),\n",
    "    ('clf',     RandomForestClassifier(n_estimators=500, random_state=42, n_jobs=-1))\n",
    "])\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "\n",
    "# 2) Extract the fitted RF and feature names\n",
    "rf = rf_pipe.named_steps['clf']\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# 3a) Tree-based importances\n",
    "importances = rf.feature_importances_\n",
    "feat_imp = pd.Series(importances, index=feature_names)\n",
    "feat_imp = feat_imp.sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 features by RF impurity‐based importance:\")\n",
    "print(feat_imp.head(10))\n",
    "\n",
    "# (Optional) bar-plot\n",
    "plt.figure(figsize=(6,4))\n",
    "feat_imp.head(10).plot.barh()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Random Forest Feature Importances\")\n",
    "plt.show()\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# 1) Fit on training set (as before)\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "\n",
    "# 2) Compute permutation importances on the *test* set\n",
    "perm_imp = permutation_importance(\n",
    "    rf_pipe, \n",
    "    X_test,                   # <— use held-out data, not X_train\n",
    "    y_test, \n",
    "    n_repeats=20,             # more repeats → smoother estimates\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    scoring='f1_macro'        # pick the metric you care about\n",
    ")\n",
    "\n",
    "# 3) Turn into a sorted Series\n",
    "perm_series = pd.Series(\n",
    "    perm_imp.importances_mean, \n",
    "    index=X_train.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "# 4) Inspect top features\n",
    "print(\"Top 10 features by permutation importance (on test set):\")\n",
    "print(perm_series.head(10))\n",
    "\n",
    "# 5) Plot them\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "perm_series.head(10).plot.barh()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Permutation Importances (test set)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
